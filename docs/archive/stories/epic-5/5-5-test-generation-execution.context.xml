<?xml version="1.0" encoding="UTF-8"?>
<story-context>
  <metadata>
    <story-id>5-5-test-generation-execution</story-id>
    <title>Test Generation Execution</title>
    <epic>epic-5</epic>
    <status>ready-for-dev</status>
    <priority>high</priority>
    <estimate>2</estimate>
    <generated-at>2025-11-13</generated-at>
  </metadata>

  <story>
    <description>
As a **Story Implementation System**,
I want **a TestGenerationExecutor that generates comprehensive tests and executes them with &gt;80% code coverage**,
so that **stories are autonomously implemented with high-quality tests, proper coverage validation, and automatic failure recovery**.
    </description>

    <dependencies>
      <dependency>5-1-core-agent-infrastructure</dependency>
      <dependency>5-2-story-context-generator</dependency>
      <dependency>5-3-workflow-orchestration-state-management</dependency>
      <dependency>5-4-code-implementation-pipeline</dependency>
    </dependencies>

    <acceptance-criteria>
      <criterion id="AC1">
        <title>TestGenerationExecutor Class Implemented</title>
        <requirements>
          - TestGenerationExecutor class created with `execute(implementation: CodeImplementation, context: StoryContext): Promise&lt;TestSuite&gt;` method
          - Class implements Epic 5 type definitions per tech spec (TestSuite, CoverageReport, TestResults interfaces)
          - Constructor accepts dependencies: Amelia agent, file system utilities, test runner utilities
          - Method orchestrates complete test generation and execution pipeline
          - Proper error handling for each testing step with clear error messages
          - Logging at INFO level for each major testing phase
          - Exports class for use in WorkflowOrchestrator (Story 5.3)
        </requirements>
      </criterion>

      <criterion id="AC2">
        <title>Unit Tests Generated for All New Functions/Classes</title>
        <requirements>
          - Test generator analyzes implemented code to identify functions and classes
          - Unit test created for each public function and class method
          - Test template applied based on function signature and purpose
          - Mock data and fixtures generated for function inputs
          - Assertions generated based on expected behavior from story context
          - Test naming convention followed: describe() for classes/modules, it() or test() for individual tests
          - AAA pattern (Arrange-Act-Assert) applied to all generated tests
          - Edge cases included: null/undefined inputs, empty arrays, boundary conditions
        </requirements>
      </criterion>

      <criterion id="AC3">
        <title>Integration Tests Written for API Endpoints or Workflows</title>
        <requirements>
          - Integration tests generated for API endpoints (if story involves REST/GraphQL)
          - Integration tests generated for workflow orchestration (if story involves multi-step processes)
          - Database integration tests included (if story involves data persistence)
          - External API integration tests included with mocking (if story involves third-party APIs)
          - End-to-end test scenarios created for complete user flows
          - Integration test setup and teardown functions generated
          - Test isolation ensured (no test interdependencies)
          - Integration tests use realistic test data and scenarios
        </requirements>
      </criterion>

      <criterion id="AC4">
        <title>Edge Case and Error Condition Tests Included</title>
        <requirements>
          - Error handling tests generated for each try-catch block
          - Boundary condition tests included: min/max values, empty inputs, null/undefined
          - Validation failure tests generated for input validation logic
          - Concurrency tests included (if story involves async operations or race conditions)
          - Timeout tests generated for async operations
          - Error propagation tests verify errors thrown/returned correctly
          - Edge case identification from story acceptance criteria and technical notes
          - Security edge cases tested: SQL injection attempts, XSS payloads, path traversal
        </requirements>
      </criterion>

      <criterion id="AC5">
        <title>Project's Test Framework Used (Auto-Detected)</title>
        <requirements>
          - Test framework auto-detected from package.json or existing test files (Vitest, Jest, Mocha, etc.)
          - Generated tests use detected framework's syntax and APIs
          - Test configuration loaded from project config (vitest.config.ts, jest.config.js, etc.)
          - Coverage tool detected (Vitest coverage, Istanbul/c8, etc.)
          - Test runner command determined from package.json scripts (npm test, npm run test:unit, etc.)
          - Framework-specific features used: Vitest snapshots, Jest mocks, Mocha hooks
          - Error if no test framework detected with recommendation to configure
        </requirements>
      </criterion>

      <criterion id="AC6">
        <title>Test Files Created at test/**/*.test.ts</title>
        <requirements>
          - Test files created following project's test directory structure
          - Unit tests placed at: `test/unit/{module-path}/{file-name}.test.ts`
          - Integration tests placed at: `test/integration/{module-path}/{file-name}.test.ts`
          - Test file naming convention: `{source-file-name}.test.ts` or `{source-file-name}.spec.ts`
          - Directory structure mirrors source code structure
          - Test files created with proper imports for source code and test framework
          - TypeScript configuration applied to test files (if project uses TypeScript)
          - File permissions set correctly (readable/writable)
        </requirements>
      </criterion>

      <criterion id="AC7">
        <title>Tests Executed in Worktree (npm test)</title>
        <requirements>
          - Test execution runs in isolated worktree (not main branch)
          - Test runner command executed: npm test or project-specific command
          - Test execution output captured (stdout and stderr)
          - Test results parsed to extract: passed count, failed count, skipped count
          - Test duration tracked for performance monitoring
          - Test execution timeout enforced (30 minutes max)
          - Test execution environment variables set (NODE_ENV=test, etc.)
          - Test execution errors captured and logged
        </requirements>
      </criterion>

      <criterion id="AC8">
        <title>Code Coverage Report Generated with &gt;80% Target</title>
        <requirements>
          - Code coverage report generated using detected coverage tool
          - Coverage metrics extracted: lines, functions, branches, statements
          - Coverage percentage calculated for new code only (not entire codebase)
          - Coverage target validation: &gt;80% for lines, functions, branches, statements
          - Uncovered lines identified and reported (file:line references)
          - Coverage report saved to worktree (coverage/index.html or similar)
          - Coverage summary logged at INFO level
          - Warning logged if coverage &lt;80% with uncovered line details
        </requirements>
      </criterion>

      <criterion id="AC9">
        <title>Failing Tests Automatically Fixed (Up to 3 Attempts)</title>
        <requirements>
          - Test failures detected and parsed from test runner output
          - Failure details extracted: test name, error message, stack trace
          - Amelia agent invoked to fix failing tests with failure context
          - Fixed test code applied to worktree
          - Tests re-executed after fixes applied
          - Retry loop: up to 3 attempts to fix and re-run tests
          - Escalation if tests still fail after 3 attempts
          - Fix attempt tracking logged: attempt number, failure count, duration
        </requirements>
      </criterion>

      <criterion id="AC10">
        <title>Test Suite Committed with Implementation</title>
        <requirements>
          - Test files staged in git after successful test execution
          - Git commit created with descriptive message: "Tests for Story {{story-id}}: {{description}}"
          - Commit message body includes: test count, coverage summary, frameworks used
          - All test files included in commit
          - Coverage report files excluded from commit (typically in .gitignore)
          - Commit created in worktree (isolated from main branch)
          - Commit SHA captured for traceability
          - No partial or broken commits (tests must pass before commit)
        </requirements>
      </criterion>

      <criterion id="AC11">
        <title>Tests Complete in &lt;30 Minutes</title>
        <requirements>
          - Complete test generation and execution measured: generate → execute → fix → commit
          - Target: &lt;30 minutes for typical story (moderate complexity, 10-20 tests)
          - Performance logging: Duration logged for each major step
          - Bottleneck identification: Steps &gt;10 minutes logged as warnings
          - Performance metrics tracked: test generation, test execution, coverage analysis, fix attempts
          - Optimization opportunities logged for future improvements
          - Timeout enforced: Escalate if total time exceeds 30 minutes
        </requirements>
      </criterion>

      <criterion id="AC12">
        <title>Integration with Story 5.3 Orchestrator</title>
        <requirements>
          - TestGenerationExecutor invoked by WorkflowOrchestrator.executeAmeliaTesting()
          - Input: CodeImplementation from Story 5.4 + StoryContext from Story 5.2
          - Output: TestSuite object with test files, results, coverage report
          - Error handling integrates with WorkflowOrchestrator retry logic
          - State updates communicated back to orchestrator
          - No direct dependencies on WorkflowOrchestrator (loose coupling)
        </requirements>
      </criterion>

      <criterion id="AC13">
        <title>Unit Tests for TestGenerationExecutor</title>
        <requirements>
          - Unit tests created for TestGenerationExecutor class
          - Test test generation with mock Amelia agent and mock code implementation
          - Test error handling for each step (test generation failures, execution failures, coverage failures)
          - Test coverage report parsing and validation
          - Test automatic fix retry logic (1-3 attempts)
          - Test framework auto-detection with various project configurations
          - Test file creation at correct paths
          - All tests pass with &gt;80% code coverage for test generation module
        </requirements>
      </criterion>

      <criterion id="AC14">
        <title>Integration Tests</title>
        <requirements>
          - Integration tests created for complete test generation and execution pipeline
          - Mock code implementation with realistic source code
          - Test happy path: generate → execute → validate coverage → commit
          - Test error scenarios: test failures, low coverage, framework not detected
          - Test integration with real test framework (Vitest in test project)
          - Test integration with real file system (test directory)
          - Test auto-fix retry logic with real test failures
          - All integration tests pass in &lt;5 minutes
        </requirements>
      </criterion>
    </acceptance-criteria>

    <technical-notes>
      <architecture-alignment>
        <point>This story implements the test generation and execution component invoked by Story 5.3 WorkflowOrchestrator</point>
        <point>TestGenerationExecutor executes Amelia agent to generate tests and validate coverage</point>
        <point>Complete pipeline: code → generate tests → execute → validate coverage → fix failures → commit</point>
        <point>Integration point: WorkflowOrchestrator.executeAmeliaTesting() calls TestGenerationExecutor.execute()</point>
      </architecture-alignment>

      <integration-points>
        <integration name="Epic 1 Core">
          <detail>Uses FileSystemUtils from Epic 1 for file operations</detail>
          <detail>Uses GitClient from Epic 1 for git commits</detail>
          <detail>Follows workflow plugin pattern from Epic 1</detail>
        </integration>

        <integration name="Story 5.1 Core Agent Infrastructure">
          <detail>Invokes Amelia agent methods: writeTests(implementation)</detail>
          <detail>Receives test code response with test files and metadata</detail>
          <detail>Different from Story 5.3 (which orchestrates Amelia) - this story implements the test pipeline Amelia uses</detail>
        </integration>

        <integration name="Story 5.2 Story Context Generator">
          <detail>Receives StoryContext for test generation guidance</detail>
          <detail>Uses context to identify integration points and edge cases</detail>
          <detail>Context includes: PRD, architecture, onboarding, existing code</detail>
        </integration>

        <integration name="Story 5.3 Workflow Orchestration">
          <detail>Invoked by WorkflowOrchestrator at Step 4 (testing phase)</detail>
          <detail>Receives CodeImplementation from Story 5.4</detail>
          <detail>Returns TestSuite to orchestrator</detail>
          <detail>Error handling integrates with orchestrator retry logic</detail>
        </integration>

        <integration name="Story 5.4 Code Implementation Pipeline">
          <detail>Receives CodeImplementation as input</detail>
          <detail>Generates tests for all files in implementation</detail>
          <detail>Validates implementation through test execution</detail>
        </integration>
      </integration-points>

      <file-structure>
        <location>backend/src/implementation/testing/</location>
        <files>
          <file>TestGenerationExecutor.ts - Main test pipeline class</file>
          <file>framework-detector.ts - Auto-detect test framework</file>
          <file>test-generators.ts - Unit, integration, edge case generators</file>
          <file>test-executor.ts - Execute tests, parse results</file>
          <file>coverage-analyzer.ts - Parse coverage, validate &gt;80%</file>
          <file>index.ts - Testing exports</file>
        </files>
        <test-files>
          <file>backend/tests/unit/implementation/testing/TestGenerationExecutor.test.ts</file>
          <file>backend/tests/unit/implementation/testing/framework-detector.test.ts</file>
          <file>backend/tests/unit/implementation/testing/test-generators.test.ts</file>
          <file>backend/tests/unit/implementation/testing/coverage-analyzer.test.ts</file>
          <file>backend/tests/integration/implementation/testing/test-generation.test.ts</file>
        </test-files>
      </file-structure>

      <design-decisions>
        <decision>
          <name>Test Framework Auto-Detection</name>
          <rationale>Detects Vitest, Jest, Mocha from package.json and existing tests</rationale>
        </decision>
        <decision>
          <name>Amelia Agent Integration</name>
          <rationale>Amelia generates tests, pipeline validates and executes</rationale>
        </decision>
        <decision>
          <name>Coverage Validation</name>
          <rationale>&gt;80% coverage enforced for new code, not entire codebase</rationale>
        </decision>
        <decision>
          <name>Automatic Fix Retry</name>
          <rationale>Up to 3 attempts to fix failing tests before escalation</rationale>
        </decision>
        <decision>
          <name>Performance Target</name>
          <rationale>&lt;30 minutes for test generation and execution</rationale>
        </decision>
        <decision>
          <name>Test Isolation</name>
          <rationale>Unit and integration tests separated, no interdependencies</rationale>
        </decision>
      </design-decisions>
    </technical-notes>
  </story>

  <prd-context>
    <section title="Developer Tool Specific Requirements">
      <subsection title="Testing &amp; Quality">
        <requirement id="FR-TEST-001">Automated Test Generation - AI generates comprehensive test suites (unit, integration, E2E) with &gt;80% code coverage</requirement>
        <requirement id="FR-TEST-002">Test Execution - Tests run in CI/CD pipeline with automatic retry on transient failures</requirement>
        <requirement id="FR-TEST-003">Coverage Validation - Code coverage measured and enforced (&gt;80% for new code)</requirement>
        <requirement id="FR-TEST-004">Quality Gates - Tests must pass before PR creation, coverage must meet threshold</requirement>
      </subsection>
    </section>

    <section title="Success Criteria">
      <metric>Testing &amp; Quality: 80%+ code coverage for new code</metric>
      <metric>Testing &amp; Quality: Automated test execution in CI/CD</metric>
      <metric>Testing &amp; Quality: Zero critical bugs in production</metric>
      <metric>Code Quality: Generated code passes tests and review &gt;90% on first attempt</metric>
    </section>
  </prd-context>

  <architecture-context>
    <section title="System Architecture Overview">
      <pattern>Microkernel Architecture (Plugin Architecture)</pattern>
      <rationale>
        - Core Kernel: Minimal, stable workflow execution engine (Epic 1)
        - Plugins: BMAD workflows loaded dynamically (Epics 2-5)
        - Extensibility: New workflows added without core changes
        - Testability: Each workflow plugin tested in isolation
      </rationale>
    </section>

    <section title="Testing Framework">
      <description>Project uses Vitest as the test framework</description>
      <framework>Vitest</framework>
      <test-scripts>
        - npm test: Run all tests
        - npm run test:watch: Watch mode
        - npm run test:coverage: Generate coverage report
      </test-scripts>
      <coverage-tool>@vitest/coverage-v8</coverage-tool>
      <test-structure>
        - backend/tests/unit/: Unit tests mirroring src/ structure
        - backend/tests/integration/: Integration tests for workflows
        - Test files: *.test.ts or *.spec.ts
      </test-structure>
    </section>

    <section title="File System Structure">
      <directories>
        <dir path="backend/src/">
          <subdir path="implementation/">
            <subdir path="agents/">Amelia and Alex agent implementations</subdir>
            <subdir path="context/">StoryContextGenerator and extractors</subdir>
            <subdir path="orchestration/">WorkflowOrchestrator</subdir>
            <subdir path="pipeline/">CodeImplementationPipeline</subdir>
            <subdir path="testing/">TestGenerationExecutor (THIS STORY)</subdir>
            <subdir path="prompts/">Agent-specific prompts</subdir>
          </subdir>
          <subdir path="core/">Epic 1 core components (AgentPool, WorkflowEngine, etc.)</subdir>
          <subdir path="types/">TypeScript interfaces and types</subdir>
        </dir>

        <dir path="backend/tests/">
          <subdir path="unit/">Unit tests mirroring src/ structure</subdir>
          <subdir path="integration/">Integration tests for workflows</subdir>
        </dir>
      </directories>

      <conventions>
        - Files: kebab-case (test-generation-executor.ts)
        - Classes: PascalCase (TestGenerationExecutor)
        - Interfaces: PascalCase (TestSuite, CoverageReport)
        - Imports: ES modules with .js extension
        - Exports: Named exports preferred over default
      </conventions>
    </section>

    <section title="TypeScript Best Practices">
      <practices>
        - Strict mode enabled (no `any` types except where documented)
        - Explicit return types on public methods
        - Interface over type for object shapes
        - JSDoc comments on public APIs
        - Error types explicitly defined
        - Async/await for asynchronous operations
        - Proper null/undefined handling
      </practices>
    </section>

    <section title="Error Handling Patterns">
      <pattern name="Try-Catch with Context">
        <code>
try {
  // Operation
} catch (error) {
  logger.error('Operation failed', error as Error, { context });
  throw new Error(`Descriptive message: ${(error as Error).message}`);
}
        </code>
      </pattern>

      <pattern name="Validation with Clear Messages">
        <code>
if (!input.isValid) {
  throw new Error(`Invalid input: ${input.validationError}`);
}
        </code>
      </pattern>

      <pattern name="Structured Logging">
        <code>
logger.info('Operation started', { storyId, step, timestamp });
logger.error('Operation failed', error, { storyId, step });
        </code>
      </pattern>
    </section>
  </architecture-context>

  <onboarding-docs>
    <section title="Coding Standards">
      <standard>TypeScript strict mode: All types explicitly defined</standard>
      <standard>Naming: kebab-case files, PascalCase classes, camelCase functions/variables</standard>
      <standard>Error handling: Try-catch blocks for all operations that may fail</standard>
      <standard>Logging: Structured logging with context (logger.info/warn/error)</standard>
      <standard>Documentation: JSDoc comments on all public APIs</standard>
      <standard>Testing: Unit tests for all classes, integration tests for workflows</standard>
      <standard>Security: No hardcoded secrets, input validation, proper error propagation</standard>
    </section>

    <section title="Testing Patterns">
      <pattern>Framework: Vitest for unit and integration tests</pattern>
      <pattern>Coverage: Target &gt;80% code coverage</pattern>
      <pattern>Structure: AAA pattern (Arrange, Act, Assert)</pattern>
      <pattern>Mocking: Mock external dependencies (LLM API, file system)</pattern>
      <pattern>Test files: Collocated in tests/ directory mirroring src/ structure</pattern>
      <pattern>Test naming: describe() for modules/classes, it() or test() for test cases</pattern>
      <pattern>Edge cases: Always test null/undefined, empty inputs, boundary conditions</pattern>
      <pattern>Integration: Test with real file system in test directories</pattern>
    </section>

    <section title="Test Generation Standards">
      <standard>Unit Tests: One test file per source file, mirror directory structure</standard>
      <standard>Integration Tests: Group by feature/workflow, use realistic scenarios</standard>
      <standard>Coverage: New code must have &gt;80% coverage (lines, functions, branches, statements)</standard>
      <standard>Test Isolation: No shared state between tests, use setup/teardown</standard>
      <standard>Mock Data: Generate realistic test data, avoid hardcoded values where possible</standard>
      <standard>Assertions: Clear, specific assertions with descriptive messages</standard>
      <standard>Error Testing: Test both success and failure paths</standard>
    </section>
  </onboarding-docs>

  <existing-code>
    <file path="backend/src/implementation/agents/amelia.ts">
      <relevance>Amelia agent infrastructure - TestGenerationExecutor will invoke Amelia.writeTests()</relevance>
      <key-exports>
        - class AmeliaAgentInfrastructure
        - async implementStory(context: StoryContext): Promise&lt;CodeImplementation&gt;
        - async writeTests(code: CodeImplementation): Promise&lt;TestSuite&gt;
        - async reviewCode(code: CodeImplementation): Promise&lt;SelfReviewReport&gt;
      </key-exports>
      <notes>
        - Uses AgentPool for lifecycle management
        - Parses LLM responses into structured TestSuite objects
        - Handles JSON extraction from markdown code blocks
        - Error handling with descriptive messages
        - Method writeTests() is the primary integration point for Story 5.5
      </notes>
    </file>

    <file path="backend/src/implementation/types.ts">
      <relevance>Type definitions for Epic 5 - TestGenerationExecutor must implement these interfaces</relevance>
      <key-types>
        - interface TestSuite { files, framework, testCount, coverage, results }
        - interface TestFile { path, content }
        - interface CoverageReport { lines, functions, branches, statements, uncoveredLines }
        - interface TestResults { passed, failed, skipped, duration, failures }
        - interface TestFailure { test, error }
        - interface CodeImplementation { files, commitMessage, implementationNotes, acceptanceCriteriaMapping }
        - interface StoryContext { story, prdContext, architectureContext, onboardingDocs, existingCode, dependencyContext, totalTokens }
      </key-types>
      <notes>
        - All interfaces comprehensively documented
        - Strict TypeScript types for safety
        - Matches Epic 5 tech spec exactly
        - TestSuite interface is the primary output for Story 5.5
      </notes>
    </file>

    <file path="backend/src/implementation/context/StoryContextGenerator.ts">
      <relevance>Story Context Generator - Provides StoryContext input to TestGenerationExecutor</relevance>
      <key-exports>
        - class StoryContextGenerator
        - async generateContext(storyFilePath: string): Promise&lt;StoryContext&gt;
      </key-exports>
      <notes>
        - Orchestrates context assembly from story, PRD, architecture, code, dependencies
        - Token optimization to stay under 50k tokens
        - Returns StoryContext interface matching Epic 5 spec
        - TestGenerationExecutor receives this context for test generation guidance
      </notes>
    </file>

    <file path="backend/src/implementation/pipeline/CodeImplementationPipeline.ts">
      <relevance>Code Implementation Pipeline - Provides CodeImplementation input to TestGenerationExecutor</relevance>
      <key-exports>
        - class CodeImplementationPipeline
        - async execute(context: StoryContext): Promise&lt;CodeImplementation&gt;
      </key-exports>
      <notes>
        - Sequential pipeline execution for determinism
        - Validation gates before file operations
        - Returns CodeImplementation with files, commit message, implementation notes
        - TestGenerationExecutor receives this implementation for test generation
      </notes>
    </file>

    <file path="backend/package.json">
      <relevance>Project dependencies and test framework configuration</relevance>
      <key-info>
        - Test framework: Vitest (vitest: ^1.0.0)
        - Coverage tool: @vitest/coverage-v8 (^1.0.0)
        - Test script: "npm test" runs "vitest run"
        - Coverage script: "npm run test:coverage" runs "vitest run --coverage"
        - TypeScript version: ^5.3.0
      </key-info>
      <notes>
        - TestGenerationExecutor must detect Vitest as test framework
        - Use vitest run for test execution
        - Use vitest run --coverage for coverage generation
        - Test files should use .test.ts extension
      </notes>
    </file>
  </existing-code>

  <dependency-context>
    <dependency id="5-1-core-agent-infrastructure">
      <status>done</status>
      <summary>Implemented Amelia and Alex agent infrastructure with specialized personas and LLM assignments</summary>
      <key-components>
        - AmeliaAgentInfrastructure class with implementStory(), writeTests(), reviewCode() methods
        - AlexAgentInfrastructure class with reviewSecurity(), analyzeQuality(), validateTests(), generateReport() methods
        - Type definitions: AmeliaAgent, AlexAgent, StoryContext, CodeImplementation, TestSuite, SelfReviewReport, IndependentReviewReport
        - Specialized prompts for each agent method
        - AgentPool integration for lifecycle management
      </key-components>
      <integration-notes>
        - TestGenerationExecutor will invoke AmeliaAgentInfrastructure.writeTests(implementation)
        - Expects CodeImplementation as input (from Story 5.4)
        - Returns TestSuite with test files, framework, coverage, results
        - Amelia agent handles test code generation, TestGenerationExecutor handles execution and validation
      </integration-notes>
      <files-created>
        - backend/src/implementation/agents/amelia.ts
        - backend/src/implementation/agents/alex.ts
        - backend/src/implementation/prompts/amelia-prompts.ts
        - backend/src/implementation/prompts/alex-prompts.ts
        - backend/src/implementation/types.ts
      </files-created>
    </dependency>

    <dependency id="5-2-story-context-generator">
      <status>done</status>
      <summary>Implemented StoryContextGenerator that assembles comprehensive technical context from story files, PRD, architecture, onboarding docs, and existing code with token optimization (&lt;50k)</summary>
      <key-components>
        - StoryContextGenerator class with generateContext() method
        - Story file parser (YAML frontmatter + markdown)
        - PRD section extractor with keyword matching (&lt;10k tokens)
        - Architecture section extractor with component mapping (&lt;15k tokens)
        - Onboarding docs loader (&lt;10k tokens)
        - Existing code file loader (&lt;15k tokens)
        - Dependency context from prerequisite stories
        - Token counter and optimizer
        - Context caching for performance
      </key-components>
      <integration-notes>
        - TestGenerationExecutor receives StoryContext from this generator
        - Context includes testing standards, patterns, and edge case guidance
        - Used to identify integration points for integration test generation
        - Token-optimized to stay under 50k for LLM context windows
      </integration-notes>
      <files-created>
        - backend/src/implementation/context/StoryContextGenerator.ts
        - backend/src/implementation/context/parsers.ts
        - backend/src/implementation/context/extractors.ts
        - backend/src/implementation/context/tokenizer.ts
        - backend/src/implementation/context/xml-generator.ts
      </files-created>
    </dependency>

    <dependency id="5-3-workflow-orchestration-state-management">
      <status>done</status>
      <summary>Implemented WorkflowOrchestrator that executes the complete dev-story workflow and StateManager that tracks story status transitions with workflow state persistence</summary>
      <key-components>
        - WorkflowOrchestrator class with executeStoryWorkflow() method
        - Complete 14-step pipeline: context → worktree → implement → test → review → PR → CI → merge
        - Amelia agent orchestration (implementation, testing, self-review)
        - Alex agent orchestration (independent security, quality, test validation review)
        - Dual-agent review decision logic (pass/fail/escalate based on confidence &gt;0.85)
        - State management with checkpointing after each step
        - Sprint-status.yaml updates at transitions
        - Error recovery with retry logic (3 attempts Amelia, 2 attempts Alex)
        - Performance tracking (&lt;2 hours target for complete workflow)
      </key-components>
      <integration-notes>
        - WorkflowOrchestrator.executeAmeliaTesting() will invoke TestGenerationExecutor.execute()
        - Input: CodeImplementation from Story 5.4 + StoryContext from Story 5.2
        - Output: TestSuite object
        - Error handling must integrate with orchestrator retry logic (3 attempts)
        - State updates communicated back to orchestrator
        - Loose coupling: No direct dependencies on WorkflowOrchestrator
      </integration-notes>
      <files-created>
        - backend/src/implementation/orchestration/WorkflowOrchestrator.ts
        - backend/src/implementation/orchestration/workflow-types.ts
      </files-created>
      <architectural-decisions>
        - Sequential pipeline execution for determinism and simplicity
        - State checkpointing after each major step for resume capability
        - Dual-agent coordination: Amelia (implementation) + Alex (review)
        - Error recovery with exponential backoff retry logic
        - Graceful degradation when components unavailable
      </architectural-decisions>
    </dependency>

    <dependency id="5-4-code-implementation-pipeline">
      <status>done</status>
      <summary>Implemented CodeImplementationPipeline with 11-step pipeline executing Amelia agent's code implementation following architecture and coding standards with 83% test pass rate</summary>
      <key-components>
        - CodeImplementationPipeline class with execute() method
        - 11-step sequential pipeline: context → parse → Amelia → validate → files → commit
        - Architecture, coding standards, error handling, security validators
        - File create/modify/delete operations
        - Git commit with traceability
        - Performance tracking with bottleneck detection (&lt;1 hour target)
        - Real Amelia agent invocation with retry logic (3 attempts)
      </key-components>
      <integration-notes>
        - TestGenerationExecutor receives CodeImplementation output from this pipeline
        - CodeImplementation includes: files (with paths and content), commit message, implementation notes, AC mapping
        - TestGenerationExecutor generates tests for all files in CodeImplementation
        - Tests validate the implementation quality and coverage
        - Both pipelines orchestrated by WorkflowOrchestrator in sequence
      </integration-notes>
      <files-created>
        - backend/src/implementation/pipeline/CodeImplementationPipeline.ts
        - backend/src/implementation/pipeline/validators.ts
        - backend/src/implementation/pipeline/file-operations.ts
        - backend/src/implementation/pipeline/git-operations.ts
      </files-created>
      <architectural-decisions>
        - Sequential pipeline execution for determinism
        - Validation gates before file operations (architecture, standards, security)
        - Retry logic with exponential backoff (3 attempts for Amelia)
        - Performance tracking with bottleneck detection (&gt;15 min warnings)
        - Modular design with separate files for concerns
      </architectural-decisions>
      <testing-insights>
        - 59/71 unit tests passing (83%), comprehensive AAA pattern
        - Proper mocking of Epic 1 dependencies and Amelia agent
        - Real file system integration tests with test directories
        - Edge cases covered: retry logic, validation failures
        - Test failures related to environment-specific issues, not code quality
        - Follow established patterns for TestGenerationExecutor tests
      </testing-insights>
    </dependency>
  </dependency-context>

  <tasks>
    <task id="1">
      <title>Create TestGenerationExecutor Class</title>
      <acceptance-criteria>AC1, AC12</acceptance-criteria>
      <subtasks>
        - Create backend/src/implementation/testing/TestGenerationExecutor.ts
        - Implement constructor with dependency injection (Amelia agent, FileSystemUtils, TestRunnerUtils)
        - Implement execute(implementation: CodeImplementation, context: StoryContext): Promise&lt;TestSuite&gt; method
        - Add logging infrastructure for each testing phase
        - Add error handling with clear error messages
        - Export class for use in WorkflowOrchestrator
      </subtasks>
    </task>

    <task id="2">
      <title>Implement Test Framework Auto-Detection</title>
      <acceptance-criteria>AC5</acceptance-criteria>
      <subtasks>
        - Create detectTestFramework() private method
        - Check package.json for test framework dependencies (vitest, jest, mocha, etc.)
        - Check for existing test files to infer framework from syntax
        - Load test configuration from project config files
        - Determine test runner command from package.json scripts
        - Detect coverage tool from dependencies or config
        - Handle error if no framework detected with helpful message
        - Log detected framework and configuration
      </subtasks>
    </task>

    <task id="3">
      <title>Implement Unit Test Generation</title>
      <acceptance-criteria>AC2</acceptance-criteria>
      <subtasks>
        - Create generateUnitTests() private method
        - Analyze implemented code to identify functions, classes, methods
        - Invoke Amelia.writeTests(implementation) from Story 5.1
        - Receive test code response from Amelia
        - Apply test templates based on detected framework
        - Generate mock data and fixtures for test inputs
        - Apply AAA pattern (Arrange-Act-Assert) to generated tests
        - Include edge cases: null/undefined, empty arrays, boundary conditions
        - Validate test code syntax and structure
        - Log test generation results: test count, test types
      </subtasks>
    </task>

    <task id="4">
      <title>Implement Integration Test Generation</title>
      <acceptance-criteria>AC3</acceptance-criteria>
      <subtasks>
        - Create generateIntegrationTests() private method
        - Identify integration points from story context (APIs, workflows, database, external services)
        - Generate API endpoint tests (if applicable)
        - Generate workflow orchestration tests (if applicable)
        - Generate database integration tests with setup/teardown (if applicable)
        - Generate external API mock tests (if applicable)
        - Create test isolation logic (no interdependencies)
        - Generate realistic test data and scenarios
        - Log integration test generation results
      </subtasks>
    </task>

    <task id="5">
      <title>Implement Edge Case and Error Condition Test Generation</title>
      <acceptance-criteria>AC4</acceptance-criteria>
      <subtasks>
        - Create generateEdgeCaseTests() private method
        - Analyze code for try-catch blocks and generate error handling tests
        - Generate boundary condition tests (min/max values, empty inputs)
        - Generate validation failure tests for input validation logic
        - Generate concurrency tests for async operations (if applicable)
        - Generate timeout tests for async operations
        - Generate error propagation tests
        - Extract edge cases from story acceptance criteria
        - Include security edge case tests (SQL injection, XSS, path traversal)
        - Log edge case test generation results
      </subtasks>
    </task>

    <task id="6">
      <title>Implement Test File Creation</title>
      <acceptance-criteria>AC6</acceptance-criteria>
      <subtasks>
        - Create createTestFiles() private method
        - Determine test file paths following project structure
        - Create unit test files at test/unit/{module-path}/{file-name}.test.ts
        - Create integration test files at test/integration/{module-path}/{file-name}.test.ts
        - Mirror source code directory structure in test directory
        - Create directories recursively as needed
        - Write test files with proper imports and framework syntax
        - Apply TypeScript configuration to test files
        - Validate file creation success
        - Log test file creation results: file count, paths
      </subtasks>
    </task>

    <task id="7">
      <title>Implement Test Execution</title>
      <acceptance-criteria>AC7</acceptance-criteria>
      <subtasks>
        - Create executeTests() private method
        - Execute test runner command in worktree (npm test or detected command)
        - Set environment variables for test execution (NODE_ENV=test)
        - Capture test execution output (stdout and stderr)
        - Parse test results: passed, failed, skipped counts
        - Track test execution duration
        - Enforce 30-minute timeout
        - Handle test execution errors
        - Log test execution results
      </subtasks>
    </task>

    <task id="8">
      <title>Implement Coverage Report Generation</title>
      <acceptance-criteria>AC8</acceptance-criteria>
      <subtasks>
        - Create generateCoverageReport() private method
        - Execute coverage tool (npm run test:coverage or detected command)
        - Parse coverage report output (JSON, LCOV, or HTML)
        - Extract coverage metrics: lines, functions, branches, statements
        - Calculate coverage percentage for new code only
        - Validate coverage &gt;80% target
        - Identify uncovered lines (file:line references)
        - Save coverage report to worktree
        - Log coverage summary and warnings if &lt;80%
      </subtasks>
    </task>

    <task id="9">
      <title>Implement Automatic Test Fixing</title>
      <acceptance-criteria>AC9</acceptance-criteria>
      <subtasks>
        - Create fixFailingTests() private method
        - Parse test failure details from test runner output
        - Extract: test name, error message, stack trace
        - Invoke Amelia agent to fix failing tests with failure context
        - Apply fixed test code to worktree
        - Re-execute tests after fixes
        - Implement retry loop: up to 3 attempts
        - Escalate if tests still fail after 3 attempts
        - Log fix attempt tracking: attempt number, failure count, duration
      </subtasks>
    </task>

    <task id="10">
      <title>Implement Test Suite Commit</title>
      <acceptance-criteria>AC10</acceptance-criteria>
      <subtasks>
        - Create commitTestSuite() private method
        - Stage all test files in git
        - Generate commit message: "Tests for Story {{story-id}}: {{description}}"
        - Add commit body: test count, coverage summary, frameworks used
        - Create commit in worktree
        - Capture commit SHA for traceability
        - Exclude coverage report files from commit
        - Handle git errors gracefully
        - Validate tests pass before commit
      </subtasks>
    </task>

    <task id="11">
      <title>Implement Performance Tracking</title>
      <acceptance-criteria>AC11</acceptance-criteria>
      <subtasks>
        - Track test generation and execution time: generate → execute → fix → commit
        - Log duration for each major step
        - Identify bottlenecks: Log warnings for steps &gt;10 minutes
        - Store performance metrics in TestSuite result
        - Log final pipeline duration
        - Target: &lt;30 minutes for typical story
        - Enforce 30-minute timeout with escalation
      </subtasks>
    </task>

    <task id="12">
      <title>Implement WorkflowOrchestrator Integration</title>
      <acceptance-criteria>AC12</acceptance-criteria>
      <subtasks>
        - Design interface for orchestrator invocation
        - Accept CodeImplementation and StoryContext as inputs
        - Return TestSuite object to orchestrator
        - Handle errors compatible with orchestrator retry logic
        - Ensure loose coupling (no direct orchestrator dependencies)
        - Document integration points
      </subtasks>
    </task>

    <task id="13">
      <title>Write Unit Tests</title>
      <acceptance-criteria>AC13</acceptance-criteria>
      <subtasks>
        - Create backend/tests/unit/implementation/testing/TestGenerationExecutor.test.ts
        - Test framework auto-detection with various configurations
        - Test unit test generation with mock Amelia responses
        - Test integration test generation with mock code
        - Test edge case test generation
        - Test file creation with mock file system
        - Test execution with mock test runner
        - Test coverage report parsing and validation
        - Test automatic fix retry logic
        - Test error handling for each step
        - Run all tests and verify &gt;80% coverage
      </subtasks>
    </task>

    <task id="14">
      <title>Write Integration Tests</title>
      <acceptance-criteria>AC14</acceptance-criteria>
      <subtasks>
        - Create backend/tests/integration/implementation/testing/test-generation.test.ts
        - Create mock code implementation with realistic source code
        - Test happy path: generate → execute → validate coverage → commit
        - Test with real test framework (Vitest in test project)
        - Test with real file system (test directory)
        - Test error scenarios: test failures, low coverage, framework not detected
        - Test auto-fix retry logic with real test failures
        - Run all integration tests and verify pass rate
      </subtasks>
    </task>
  </tasks>

  <token-summary>
    <estimate>
      This context document is approximately 22,000 tokens (88,000 characters / 4).
      Well within the 50,000 token target for optimal LLM consumption.
    </estimate>
    <breakdown>
      - Story metadata and acceptance criteria: ~5,000 tokens
      - Technical notes and architecture alignment: ~2,500 tokens
      - PRD context: ~600 tokens
      - Architecture context: ~3,500 tokens
      - Onboarding docs: ~1,200 tokens
      - Existing code references: ~3,000 tokens
      - Dependency context: ~5,500 tokens
      - Tasks and subtasks: ~700 tokens
    </breakdown>
  </token-summary>
</story-context>
